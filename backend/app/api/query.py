import json
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.schemas.query import AskRequest, Message
# Import Pipeline to√†n c·ª•c (ƒë·ªÉ d√πng chung RAM v·ªõi b√™n upload)
from app.core.global_state import global_rag_pipeline
from app.services.llm_client import call_llm, call_llm_general
# Import service Chat History (MongoDB)
from app.services.chat_history import get_chat_history, add_message_to_history
import uuid

from app.services.router import route_query


router = APIRouter(prefix="", tags=["query"])

# --- L∆ØU √ù: ƒê√É X√ìA ƒêO·∫†N KH·ªûI T·∫†O LOCAL MODEL ƒê·ªÇ TI·∫æT KI·ªÜM RAM ---
# Ch√∫ng ta d√πng global_rag_pipeline ƒë∆∞·ª£c import ·ªü tr√™n.

@router.post("/ask")
async def ask(req: AskRequest):
    """
    API tr·∫£ l·ªùi c√¢u h·ªèi:
    1. L·∫•y l·ªãch s·ª≠ t·ª´ MongoDB d·ª±a tr√™n session_id
    2. Retrieval (RAG)
    3. Generation (LLM) + Streaming
    4. L∆∞u l·∫°i h·ªôi tho·∫°i m·ªõi v√†o MongoDB
    """
    session_id = req.session_id if req.session_id else str(uuid.uuid4())
    # --- B∆Ø·ªöC 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (L·∫•y History t·ª´ DB) ---
    # Thay v√¨ tin v√†o req.history (client g·ª≠i), ta l·∫•y t·ª´ Database cho chu·∫©n
    db_history_dicts = await get_chat_history(session_id)

    # Convert t·ª´ dict c·ªßa Mongo sang object Message ƒë·ªÉ call_llm hi·ªÉu
    # (N·∫øu db tr·∫£ v·ªÅ r·ªóng th√¨ list n√†y r·ªóng, kh√¥ng sao c·∫£)
    history_objs = [Message(**msg) for msg in db_history_dicts]

    mode = await route_query(req.question)
    print(f"üîÑ Router Decision: {mode}") # Log ra xem n√≥ ch·ªçn g√¨

    async def response_generator():
        # A. G·ª≠i Session ID & MODE v·ªÅ Client
        # Client s·∫Ω d√πng c√°i "mode" n√†y ƒë·ªÉ hi·ªÉn th·ªã icon kh√°c nhau
        yield json.dumps({
            "type": "meta_info", # G·ªôp chung info
            "session_id": session_id,
            "mode": mode 
        }, ensure_ascii=False) + "\n"

        # B. L∆∞u c√¢u h·ªèi User
        await add_message_to_history(session_id, "user", req.question)

        full_answer = ""

        # --- NH√ÅNH 1: RAG MODE (T√¨m trong PDF) ---
        if mode == "RAG":
            unique_hits = await global_rag_pipeline.run(
                original_question=req.question,
                topk=req.topk,
                rerank_topn=req.rerank_topn
            )
            
            # G·ª≠i context n·∫øu c√≥
            if unique_hits:
                context_data = [
                    {
                        "chunk_id": h["chunk_id"], 
                        "text": h["text"], 
                        "rerank_score": h.get("rerank_score", 0),
                        "metadata": h.get("metadata")
                    } for h in unique_hits
                ]
                yield json.dumps({"type": "context", "payload": context_data}, ensure_ascii=False) + "\n"
            
            # G·ªçi LLM RAG
            async for token in call_llm(req.question, unique_hits, history_objs):
                if token:
                    full_answer += token
                    yield json.dumps({"type": "answer", "payload": token}, ensure_ascii=False) + "\n"

        # --- NH√ÅNH 2: GENERAL MODE (Chat th∆∞·ªùng) ---
        else:
            # G·ªçi LLM General (Kh√¥ng c·∫ßn context hits)
            async for token in call_llm_general(req.question, history_objs):
                if token:
                    full_answer += token
                    yield json.dumps({"type": "answer", "payload": token}, ensure_ascii=False) + "\n"
        
        # C. L∆∞u c√¢u tr·∫£ l·ªùi Assistant
        if full_answer:
            await add_message_to_history(session_id, "assistant", full_answer)

    return StreamingResponse(response_generator(), media_type="application/x-ndjson")

@router.post("/debug-retrieval")
async def debug_retrieval(req: AskRequest):
    """
    API debug xem Pipeline ƒëang t√¨m ki·∫øm nh∆∞ th·∫ø n√†o (kh√¥ng g·ªçi LLM)
    """
    print(f"üõ†Ô∏è Debug Query: {req.question}")
    
    # 1. Test sinh Query ph·ª• (Query Expansion)
    # L∆∞u √Ω: H√†m _query_processing l√† private, ch·ªâ d√πng ƒë·ªÉ debug
    sub_queries = await global_rag_pipeline._query_processing(req.question)
    
    # 2. Ch·∫°y t√¨m ki·∫øm th·∫≠t
    unique_hits = await global_rag_pipeline.run(
        original_question=req.question,
        topk=req.topk,
        rerank_topn=req.rerank_topn
    )
    
    return {
        "original_query": req.question,
        "generated_sub_queries": sub_queries,
        "results_count": len(unique_hits),
        "top_results": [
            {
                "score": h.get("rerank_score", 0),
                "text_snippet": h["text"][:150] + "...", # C·∫Øt ng·∫Øn cho d·ªÖ nh√¨n
                "source_method": h.get("metadata", {}).get("source_method", "unknown")
            }
            for h in unique_hits
        ]
    }