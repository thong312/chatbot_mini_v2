import json
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.schemas.query import AskRequest, Message
# Import Pipeline to√†n c·ª•c (ƒë·ªÉ d√πng chung RAM v·ªõi b√™n upload)
from app.core.global_state import global_rag_pipeline
from app.services.llm_client import call_llm
# Import service Chat History (MongoDB)
from app.services.chat_history import get_chat_history, add_message_to_history
import uuid
router = APIRouter(prefix="", tags=["query"])

# --- L∆ØU √ù: ƒê√É X√ìA ƒêO·∫†N KH·ªûI T·∫†O LOCAL MODEL ƒê·ªÇ TI·∫æT KI·ªÜM RAM ---
# Ch√∫ng ta d√πng global_rag_pipeline ƒë∆∞·ª£c import ·ªü tr√™n.

@router.post("/ask")
async def ask(req: AskRequest):
    """
    API tr·∫£ l·ªùi c√¢u h·ªèi:
    1. L·∫•y l·ªãch s·ª≠ t·ª´ MongoDB d·ª±a tr√™n session_id
    2. Retrieval (RAG)
    3. Generation (LLM) + Streaming
    4. L∆∞u l·∫°i h·ªôi tho·∫°i m·ªõi v√†o MongoDB
    """
    session_id = req.session_id if req.session_id else str(uuid.uuid4())
    # --- B∆Ø·ªöC 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (L·∫•y History t·ª´ DB) ---
    # Thay v√¨ tin v√†o req.history (client g·ª≠i), ta l·∫•y t·ª´ Database cho chu·∫©n
    db_history_dicts = await get_chat_history(session_id)

    # Convert t·ª´ dict c·ªßa Mongo sang object Message ƒë·ªÉ call_llm hi·ªÉu
    # (N·∫øu db tr·∫£ v·ªÅ r·ªóng th√¨ list n√†y r·ªóng, kh√¥ng sao c·∫£)
    history_objs = [Message(**msg) for msg in db_history_dicts]

    # --- B∆Ø·ªöC 2: RETRIEVAL (Search & Rerank) ---
    unique_hits = await global_rag_pipeline.run(
        original_question=req.question,
        topk=req.topk,
        rerank_topn=req.rerank_topn
    )

    # N·∫øu kh√¥ng t√¨m th·∫•y g√¨
    if not unique_hits:
        async def empty_generator():
            yield json.dumps({
                "type": "error", 
                "message": "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.", 
                "context": []
            }, ensure_ascii=False) + "\n"
        return StreamingResponse(empty_generator(), media_type="application/x-ndjson")

    # --- B∆Ø·ªöC 3: GENERATION & SAVING (Streaming) ---
    async def response_generator():
        # A. L∆∞u ngay c√¢u h·ªèi c·ªßa User v√†o DB (ƒê·ªÉ ch·∫Øc ch·∫Øn ƒë√£ ghi nh·∫≠n)
        await add_message_to_history(session_id, "user", req.question)

        # B. G·ª≠i Context cho Client
        context_data = [
            {
                "chunk_id": h["chunk_id"],
                "text": h["text"],
                "rerank_score": h.get("rerank_score", 0),
                "source": h.get("metadata", {}).get("source", "unknown"),
                "page_start": h.get("metadata", {}).get("page_start"),
                "page_end": h.get("metadata", {}).get("page_end")
            }
            for h in unique_hits
        ]
        
        yield json.dumps({
            "type": "context", 
            "payload": context_data
        }, ensure_ascii=False) + "\n"

        # C. G·ªçi LLM v√† gom c√¢u tr·∫£ l·ªùi ƒë·ªÉ l∆∞u DB
        full_answer = ""
        
        # QUAN TR·ªåNG: Truy·ªÅn history_objs (l·∫•y t·ª´ DB) v√†o ƒë√¢y
        async for token in call_llm(req.question, unique_hits, history_objs):
            if token:
                full_answer += token
                yield json.dumps({
                    "type": "answer", 
                    "payload": token
                }, ensure_ascii=False) + "\n"
        
        # D. L∆∞u c√¢u tr·∫£ l·ªùi tr·ªçn v·∫πn c·ªßa Bot v√†o DB
        if full_answer:
            await add_message_to_history(session_id, "assistant", full_answer)

    return StreamingResponse(response_generator(), media_type="application/x-ndjson")

@router.post("/debug-retrieval")
async def debug_retrieval(req: AskRequest):
    """
    API debug xem Pipeline ƒëang t√¨m ki·∫øm nh∆∞ th·∫ø n√†o (kh√¥ng g·ªçi LLM)
    """
    print(f"üõ†Ô∏è Debug Query: {req.question}")
    
    # 1. Test sinh Query ph·ª• (Query Expansion)
    # L∆∞u √Ω: H√†m _query_processing l√† private, ch·ªâ d√πng ƒë·ªÉ debug
    sub_queries = await global_rag_pipeline._query_processing(req.question)
    
    # 2. Ch·∫°y t√¨m ki·∫øm th·∫≠t
    unique_hits = await global_rag_pipeline.run(
        original_question=req.question,
        topk=req.topk,
        rerank_topn=req.rerank_topn
    )
    
    return {
        "original_query": req.question,
        "generated_sub_queries": sub_queries,
        "results_count": len(unique_hits),
        "top_results": [
            {
                "score": h.get("rerank_score", 0),
                "text_snippet": h["text"][:150] + "...", # C·∫Øt ng·∫Øn cho d·ªÖ nh√¨n
                "source_method": h.get("metadata", {}).get("source_method", "unknown")
            }
            for h in unique_hits
        ]
    }