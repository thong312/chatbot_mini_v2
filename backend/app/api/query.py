import json
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.schemas.query import AskRequest, Message
# Import Pipeline to√†n c·ª•c (ƒë·ªÉ d√πng chung RAM v·ªõi b√™n upload)
from app.core.global_state import global_rag_pipeline
from app.services.llm_client import call_llm, call_llm_general
# Import service Chat History (MongoDB)
from app.services.chat_history import get_chat_history, add_message_to_history
import uuid

from app.services.router import route_query


router = APIRouter(prefix="", tags=["query"])

# --- L∆ØU √ù: ƒê√É X√ìA ƒêO·∫†N KH·ªûI T·∫†O LOCAL MODEL ƒê·ªÇ TI·∫æT KI·ªÜM RAM ---
# Ch√∫ng ta d√πng global_rag_pipeline ƒë∆∞·ª£c import ·ªü tr√™n.

@router.post("/ask")
async def ask(req: AskRequest):
    """
    API tr·∫£ l·ªùi c√¢u h·ªèi:
    1. L·∫•y l·ªãch s·ª≠ t·ª´ MongoDB d·ª±a tr√™n session_id
    2. Retrieval (RAG)
    3. Generation (LLM) + Streaming
    4. L∆∞u l·∫°i h·ªôi tho·∫°i m·ªõi v√†o MongoDB
    """
    session_id = req.session_id if req.session_id else str(uuid.uuid4())
    # --- B∆Ø·ªöC 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (L·∫•y History t·ª´ DB) ---
    # Thay v√¨ tin v√†o req.history (client g·ª≠i), ta l·∫•y t·ª´ Database cho chu·∫©n
    db_history_dicts = await get_chat_history(session_id)

    # Convert t·ª´ dict c·ªßa Mongo sang object Message ƒë·ªÉ call_llm hi·ªÉu
    # (N·∫øu db tr·∫£ v·ªÅ r·ªóng th√¨ list n√†y r·ªóng, kh√¥ng sao c·∫£)
    history_objs = [Message(**msg) for msg in db_history_dicts]

    initial_mode = await route_query(req.question)
    print(f"üéØ Router Initial: {initial_mode}")
    

    async def response_generator():
        final_mode = initial_mode
        unique_hits = []
        
        # --- B∆Ø·ªöC QUAN TR·ªåNG: RAG FALLBACK LOGIC ---
        if initial_mode == "RAG":
            # A. Th·ª≠ t√¨m ki·∫øm trong Vector DB
            unique_hits = await global_rag_pipeline.run(
                original_question=req.question,
                topk=req.topk,
                rerank_topn=req.rerank_topn
            )

            # B. Ki·ªÉm tra ch·∫•t l∆∞·ª£ng k·∫øt qu·∫£ (Fallback)
            # Ng∆∞·ª°ng (Threshold): V·ªõi BGE-Reranker, ƒëi·ªÉm < -2 th∆∞·ªùng l√† kh√¥ng li√™n quan
            # B·∫°n c√≥ th·ªÉ ch·ªânh s·ªë -2.0 n√†y t√πy theo th·ª±c t·∫ø (VD: -1.0, 0.0)
            SCORE_THRESHOLD = -2.0 
            
            is_bad_result = False
            
            if not unique_hits:
                print("‚ö†Ô∏è RAG r·ªóng: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o.")
                is_bad_result = True
            elif unique_hits[0]['rerank_score'] < SCORE_THRESHOLD:
                print(f"‚ö†Ô∏è RAG k√©m: ƒêi·ªÉm cao nh·∫•t ch·ªâ l√† {unique_hits[0]['rerank_score']} (D∆∞·ªõi ng∆∞·ª°ng {SCORE_THRESHOLD})")
                is_bad_result = True
            
            # C. N·∫øu k·∫øt qu·∫£ t·ªá -> Chuy·ªÉn sang GENERAL
            if is_bad_result:
                final_mode = "GENERAL"
                unique_hits = [] # X√≥a k·∫øt qu·∫£ r√°c ƒë·ªÉ kh√¥ng l√†m nhi·ªÖu LLM

        # 3. G·ª≠i th√¥ng tin Mode v·ªÅ cho Client (ƒë·ªÉ hi·ªán m√†u Badge)
        yield json.dumps({
            "type": "meta_info", 
            "session_id": session_id,
            "mode": final_mode # Client s·∫Ω hi·ªÉn th·ªã General (T√≠m) ho·∫∑c RAG (Xanh) d·ª±a v√†o c√°i n√†y
        }, ensure_ascii=False) + "\n"

        # 4. L∆∞u c√¢u h·ªèi User
        await add_message_to_history(session_id, "user", req.question)

        full_answer = ""

        # --- NH√ÅNH X·ª¨ L√ù ---
        
        # TR∆Ø·ªúNG H·ª¢P 1: RAG x·ªãn (C√≥ t√†i li·ªáu ngon)
        if final_mode == "RAG" and unique_hits:
            # G·ª≠i Context
            context_data = [
                {
                    "chunk_id": h["chunk_id"], "text": h["text"], 
                    "rerank_score": h.get("rerank_score", 0), "metadata": h.get("metadata")
                } for h in unique_hits
            ]
            yield json.dumps({"type": "context", "payload": context_data}, ensure_ascii=False) + "\n"
            
            # G·ªçi LLM tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu
            async for token in call_llm(req.question, unique_hits, history_objs):
                if token:
                    full_answer += token
                    yield json.dumps({"type": "answer", "payload": token}, ensure_ascii=False) + "\n"

        # TR∆Ø·ªúNG H·ª¢P 2: GENERAL (Ho·∫∑c RAG b·ªã Fail chuy·ªÉn sang)
        else:
            # C√≥ th·ªÉ th√™m m·ªôt c√¢u th√¥ng b√°o nh·ªè n·∫øu b·ªã fallback
            if initial_mode == "RAG": 
                # N·∫øu ban ƒë·∫ßu ƒë·ªãnh t√¨m ki·∫øm m√† kh√¥ng th·∫•y, b√°o nh·∫π 1 c√¢u (t√πy ch·ªçn)
                msg = "*(Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu, t√¥i s·∫Ω tr·∫£ l·ªùi b·∫±ng ki·∫øn th·ª©c t·ªïng qu√°t...)*\n\n"
                full_answer += msg
                yield json.dumps({"type": "answer", "payload": msg}, ensure_ascii=False) + "\n"

            # G·ªçi LLM ch√©m gi√≥ (S·ª≠ d·ª•ng ki·∫øn th·ª©c training c·ªßa n√≥)
            async for token in call_llm_general(req.question, history_objs):
                if token:
                    full_answer += token
                    yield json.dumps({"type": "answer", "payload": token}, ensure_ascii=False) + "\n"

        # 5. L∆∞u c√¢u tr·∫£ l·ªùi
        if full_answer:
            await add_message_to_history(session_id, "assistant", full_answer)

    return StreamingResponse(response_generator(), media_type="application/x-ndjson")

@router.post("/debug-retrieval")
async def debug_retrieval(req: AskRequest):
    """
    API debug xem Pipeline ƒëang t√¨m ki·∫øm nh∆∞ th·∫ø n√†o (kh√¥ng g·ªçi LLM)
    """
    print(f"üõ†Ô∏è Debug Query: {req.question}")
    
    # 1. Test sinh Query ph·ª• (Query Expansion)
    # L∆∞u √Ω: H√†m _query_processing l√† private, ch·ªâ d√πng ƒë·ªÉ debug
    sub_queries = await global_rag_pipeline._query_processing(req.question)
    
    # 2. Ch·∫°y t√¨m ki·∫øm th·∫≠t
    unique_hits = await global_rag_pipeline.run(
        original_question=req.question,
        topk=req.topk,
        rerank_topn=req.rerank_topn
    )
    
    return {
        "original_query": req.question,
        "generated_sub_queries": sub_queries,
        "results_count": len(unique_hits),
        "top_results": [
            {
                "score": h.get("rerank_score", 0),
                "text_snippet": h["text"][:150] + "...", # C·∫Øt ng·∫Øn cho d·ªÖ nh√¨n
                "source_method": h.get("metadata", {}).get("source_method", "unknown")
            }
            for h in unique_hits
        ]
    }